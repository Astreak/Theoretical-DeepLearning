# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15nV8wX5_HhC6F6i1zO9mw1oJLB90wANH
"""

!kaggle competitions download -c jigsaw-toxic-comment-classification-challenge

!pip3 install -q kaggle

from google.colab import files
files.upload()

!mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/

! chmod 600 ~/.kaggle/kaggle.json

! kaggle datasets list

!kaggle competitions download -c jigsaw-toxic-comment-classification-challenge

!unzip train.csv.zip

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import torch
from torch.nn import *
from torch.nn.functional import *
import torchvision
import re
from nltk.tokenize import word_tokenize
device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device

Train=pd.read_csv('train.csv')
Train

def clean(D):
    D=D.str.lower()
    D=D.apply(lambda x:re.sub(r'\B#\S+','',x))
    D=D.apply(lambda x:re.sub(r"http\S+", " ", x))
    
    D=D.apply(lambda x:' '.join(re.findall(r'\w+', x)))
    
    D=D.apply(lambda x:re.sub(r'\s+', ' ', x, flags=re.I))
    
    D=D.apply(lambda x:re.sub(r'\s+[a-zA-Z]\s+', ' ', x))
   
    D=D.apply(lambda x:re.sub('@[^\s]+',' ',x))
    return D
def stopwords_removal(sen):
    S=stopwords.words('english')
    E=word_tokenize(sen)
    X=[w.lower() for w in E if not w in S]
    sen=" ".join(X)
    return sen

def Lemmatizing(sen):
    ps=WordNetLemmatizer()
    E=word_tokenize(sen)
    w=[ps.lemmatize(a) for a in E]
    sen=" ".join(w)
    return sen

data=Train
data['comment_text']=clean(data['comment_text'])

for i in range(len(data)):
  data['comment_text'][i]=stopwords_removal(data['comment_text'][i])
for j in range(len(data)):
  data['comment_text'][i]=Lemmatizing(data['comment_text'][i])

data['comment_text']

WI={}
W=[]
Index=[True]*(len(data))
for i in data['comment_text']:
  E=i.split()
  for a in E:
    if a not in WI.keys():
      WI[a]=len(WI)+1

seq_len=200

for a in range(len(data)):
  X=[]
  A=str(data['comment_text'][a])
  E=A.split()
  if len(E)<1:
    Index[a]=False
    continue
  if len(E)<=seq_len:
    for i in range(seq_len-len(E)):
      X.append(0)
  for b in E:
    X.append(WI.get(b,len(WI)+1))
  if len(X)>seq_len:
    X=X[:seq_len]
  if len(X)!=seq_len:
    Index[a]=False
    continue
  W.append(np.array(X))

len(W)

X=np.array(W)
X.shape

X.shape

Y=data.iloc[:,2].values.reshape(-1,1).astype(np.float32)
Y=Y[Index]
Y

from sklearn.model_selection import train_test_split
X_train,x_test,Y_train,y_test=train_test_split(X,Y,test_size=0.17,random_state=0)

!unzip glove.6B.100d.txt.zip

embed={}
with open("/content/glove.6B.100d.txt","r",encoding="utf-8") as file:
    for line in file:
        line=line.split()
        word=line[0]
        vec=np.asarray(line[1:])
        embed[word]=vec

matrix=np.random.uniform(-0.1,0.1,size=(len(WI)+2,100))
for i,j in WI.items():
  if j>=len(WI)+2:
    continue
    f=embed.get(i)
    if f is not None:
        matrix[j]=f

matrix.shape

TrainD=torch.utils.data.TensorDataset(torch.from_numpy(X_train),torch.from_numpy(Y_train))
TestD=torch.utils.data.TensorDataset(torch.from_numpy(x_test),torch.from_numpy(y_test))

TrainLoader=torch.utils.data.DataLoader(dataset=TrainD,batch_size=100,shuffle=True,drop_last=True)
TestLoader=torch.utils.data.DataLoader(dataset=TestD,batch_size=100,shuffle=True,drop_last=True)



WI['<unk>']=len(WI)+1
class ToxicityDetection(Module):
  def __init__(self,input_dim,output_dim,hidden_dim,embed_dim):
    super(ToxicityDetection,self).__init__()
    self.input_dim=input_dim
    self.output_dim=output_dim
    self.hidden_dim=hidden_dim
    self.embed_dim=embed_dim
    self.embed=Embedding(len(WI)+1,self.embed_dim)
    self.embed.weights = torch.nn.Parameter(torch.from_numpy(matrix))
    self.embed.weights.requires_grad=False

    self.lstm1=GRU(self.embed_dim,self.hidden_dim,num_layers=3,batch_first=True)
    self.dropout=Dropout(0.3)
    self.l1=Linear(self.hidden_dim,64)
    self.l2=Linear(64,64)
    self.l3=Linear(64,self.output_dim)
  def forward(self,inp,h):
    x=self.embed(inp)
    x,h=self.lstm1(x,h)
    x=x[:,-1,:]
    x=self.dropout(x)
    x=relu(self.l1(x),inplace=False)
    x=relu(self.l2(x),inplace=False)
    x=self.l3(x)
    return x,h
  def init_hidden(self):
    h0=torch.zeros((3,100,self.hidden_dim)).to(device)
    return h0

Neural=ToxicityDetection(200,1,256,64)
Neural.to(device)

for a,b in TrainLoader:
  h=Neural.init_hidden()
  print(Neural(a.to(device),h)[1].shape)
  break

epochs=10
import torch.optim as opt
optimizer=opt.Adam(Neural.parameters(),lr=0.0001)
loss_f=BCEWithLogitsLoss()
for i in range(epochs):
  Neural.train()
  L=0
  for a,b in TrainLoader:
    h=Neural.init_hidden()
    a,b=a.to(device),b.to(device)
    out,h=Neural(a,h)
    loss=loss_f(out,b)
    L+=loss.item()
    optimizer.zero_grad()
    loss.backward(retain_graph=True)
    optimizer.step()
  
  print(f"Epochs {i+1} has a loss of {L}")

Neural.eval()
D=0
Ac=0
for J,K in TestLoader:
  J,K=J.to(device),K.to(device)
  h=Neural.init_hidden()
  pred,h=Neural(J,h)
  Ac=b
  pred=torch.round(torch.sigmoid(pred))
  pred=torch.reshape(pred,(len(pred),1))
  D=pred
  lu=loss_f(pred,K.float())
  cnt=(pred==K).sum().float()
  acc=(cnt/len(J))*100

D,Ac

X=[]
sen='I will kill you all niggas.'
for i in sen.split():
  if WI.get(i,None)!=None:
    X.append(WI.get(i))
  else:
    X.append(WI.get(i.lower(),0))



F=[]
for i in range(200-len(X)):
  F.append(0)
for j in X:
  F.append(j)

F=torch.tensor(F)
F.shape

h=torch.zeros((3,1,256)).to(device)
torch.sigmoid(Neural(F.unsqueeze(0).to(device),h)[0])

X

torch.save(Neural.state_dict,'/content/ok.pt')



acc

